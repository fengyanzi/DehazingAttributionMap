# Dehazing Attribution Map (å»é›¾å½’å› å›¾)

This repository provides the official implementation of the **Dehazing Attribution Map** section from the **CVPR2025** paper: [ğŸŒ Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images](https://arxiv.org/abs/2504.09621).

- ğŸ” Visualizing image regions critical for dehazing model decisions.
- ğŸ“Š Quantifying contributions of global context information to dehazing effectiveness.
- âš™ï¸ Enhancing interpretability of deep learning-based dehazing models.

---

## **å…³äºç ”ç©¶å·¥å…·æ­£å½“ä½¿ç”¨çš„å£°æ˜** **Statement on Proper Use of Research Tools**

æˆ‘ä»¬å¼€å‘çš„DAMå·¥å…·å’ŒDehazeXLæ¨¡å‹æ—¨åœ¨**ä¿ƒè¿›**é«˜åˆ†è¾¨ç‡å›¾åƒå»é›¾é¢†åŸŸçš„**å­¦æœ¯è¿›æ­¥**ï¼Œä¸åº”è¢«ç”¨äºä»»ä½•å­¦æœ¯ä¸ç«¯è¡Œä¸ºã€‚

å›´è§‚[ğŸ¤¡å±±å¤§NIPSé¢„å°æœ¬](https://arxiv.org/abs/2505.14010),é¢…å†…SOTA,é€†å¤©èµ·é£ï¼Œè¦ä¸ç›´æ¥æŠŠæˆ‘ä»¬è®ºæ–‡æ”¹ä¸ªä½œè€…ç›´æ¥æŠ•NIPSå§ï¼Œå†™è¿™ä¹ˆå¤šè¿˜éº»çƒ¦æ‚¨äº†ã€‚

---

*The source code for the proposed **DehazeXL** model is available at [GitHub](https://github.com/CastleChen339/DehazeXL). 
The mini version of the proposed dataset **8Kdehaze** can be accessed here: [Modelscope](https://www.modelscope.cn/datasets/fengyanzi/8kdehaze_mini/) | [Hugging Face](https://huggingface.co/datasets/fengyanzi/8KDehaze_mini) | [BaiduCloud](https://pan.baidu.com/s/1ZVipOYnTR-M_xG5FZNtZPQ?pwd=4321) | [AliCloud](https://www.alipan.com/s/7AVat72s4Sk).
For the full version of **8Kdehaze**, visit: [BaiduCloud](https://pan.baidu.com/s/1-z7h-BLV7BxNg4Qp6Hi5uQ?pwd=4321).

---

![LAM Example Image](./docx/main.png)

### Quick Start ğŸš€

Before using this project, make sure all necessary dependencies are installed. You can install them via:

```bash
pip install -r requirements.txt
```

Commonly used Python libraries for deep learning included:
```plaintext
numpy
torch
opencv-python
torchvision
Pillow
matplotlib
scipy
tqdm
```

To run the project (which includes DehazeXL):

1. Download test weights from: [GitHub](https://github.com/fengyanzi/DehazingAttributionMap/releases/tag/weight)
2. Place the weights in `./model/weights/weightforDAMtest.pth`
3. Execute:

```bash
python Dehaze_Demo.py
```

### Testing Your Own Model ğŸ§ª

1. Place the hazy and clear images you wish to test in the `./clear/dehaze` folder.
2. Put your dehazing model and its weights in the `model` folder.
3. Modify the following snippet in the `Dehaze_Demo.py` file to load your model and set parameters accordingly.

```python
from model.dehaze_backbones.xt.decoders.decoder_pre import xT as DehazeXL
    # Load your own model
    model = DehazeXL().to("cuda").eval()

    modelpath = r'model/weights/weightforDAMtest.pth'
    cloudimgpath = r'data/dehaze/test1.png'
    clearingpath = r'data/dehaze/clear1.png'
    save_path = './results/test.png'
```

### Parameter Description âš™ï¸

- `--modelpath`: Path to the model weights.
- `--imgpath`: Path to the test image.
- `--w`: X-coordinate of the selected area.
- `--h`: Y-coordinate of the selected area.
- `--window_size`: Size of the selected area.
- `--fold`: Number of path integral steps. Higher values yield results closer to the true value. Default is 50, not recommended to modify.
- `--sigma`: Path integral parameter, not recommended to modify.
- `--l`: Path integral parameter, not recommended to modify.
- `--alpha`: Alpha value for blending.
- `--zoomfactor`: Image zoom factor, default is 1. If your GPU configuration is low, consider setting it to 4.
- `--kde`: Whether to use KDE for visualization (requires high computational power).
- `--output_dir`: Output directory for images.
- `--betterview`: Improves the visibility of attribution maps.

## Code References ğŸ“š

This project was inspired by and references the following open-source projects. We appreciate their contribution to the community:
- [LAM](https://github.com/fengyanzi/Local-Attribution-Map-for-Super-Resolution)

## Citation ğŸ“

If you use DAM, please cite:
```bibtex
@article{chen2025tokenize,
  title={Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images},
  author={Chen, Jiuchen and Yan, Xinyu and Xu, Qizhi and Li, Kaiqi},
  journal={arXiv preprint arXiv:2504.09621},
  year={2025}
}
```



<!-- # Dehazing Attribution Map (å»é›¾å½’å› å›¾)
This repository is an official implementation of theå»é›¾å½’å› å›¾éƒ¨åˆ† of the paper  [Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images](https://arxiv.org/abs/2504.09621)

- ğŸ” å¯è§†åŒ–å»é›¾æ¨¡å‹å†³ç­–ä¾èµ–çš„å›¾åƒåŒºåŸŸ
- ğŸ“Š é‡åŒ–å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å¯¹å»é›¾æ•ˆæœçš„è´¡çŒ®
- âš™ï¸ æ”¹å–„åŸºäºæ·±åº¦å­¦ä¹ çš„å»é›¾æ¨¡å‹å¯è§£é‡Šæ€§

---
*The source code ofè®ºæ–‡æ‰€æå‡º DehazeXL are available at [https://github.com/CastleChen339/DehazeXL](https://github.com/CastleChen339/DehazeXL).
è®ºæ–‡æ‰€æå‡ºæ•°æ®é›†8Kdehazeå¯ä»¥åœ¨ä»¥ä¸‹è·å¾—The mini version of 8Kdehaze was released: [Modelscope](https://www.modelscope.cn/datasets/fengyanzi/8kdehaze_mini/) [Hugging Face](https://huggingface.co/datasets/fengyanzi/8KDehaze_mini)  [BaiduCloud](https://pan.baidu.com/s/1ZVipOYnTR-M_xG5FZNtZPQ?pwd=4321)   [AliCloud](https://www.alipan.com/s/7AVat72s4Sk)

The Full version of 8Kdehaze:  [BaiduCloud](https://pan.baidu.com/s/1-z7h-BLV7BxNg4Qp6Hi5uQ?pwd=4321)

---

![LAM Example Image](./docx/main.png) 

### Quick Start 


åœ¨ä½¿ç”¨è¯¥é¡¹ç›®ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–åº“ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š
```bash
pip install -r requirements.txt

```

å…¶ä¸­åªåŒ…å«ä¸€äº›å¸¸è§çš„æ·±åº¦å­¦ä¹ pythonåº“ï¼š
```plaintext
numpy
torch
opencv-python
torchvision
Pillow
matplotlib
scipy
tqdm
```


è¦è¿è¡Œæœ¬é¡¹ç›®
æœ¬é¡¹ç›®å†…ç½®DehazeXL
æµ‹è¯•æƒé‡å¯ä»¥ä»ä»¥ä¸‹ä½ç½®è·å¾—ï¼šhttps://github.com/fengyanzi/DehazingAttributionMap/releases/tag/weight
é¦–å…ˆè¯·ä¸‹è½½æƒé‡å¹¶æ”¾ç½®åœ¨./model/weights/weightforDAMtest.pth

To run the project, execute:

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python Dehaze_Demo.py
```


### æµ‹è¯•è‡ªå·±çš„æ¨¡å‹

1. Place the æœ‰é›¾å’Œæ— é›¾çš„images to be tested in the `ã€‚/clear/dehaze` folder. 
2. Place the dehaze model and its weights in the `model` folder. 
3. Modify the following code in the `Dehaze_Demo.py` file to load your model and configure parameters.

```python
from model.dehaze_backbones.xt.decoders.decoder_pre import xT as DehazeXL
    # Load your own model
    model = DehazeXL().to("cuda").eval()

    modelpath = r'model/weights/weightforDAMtest.pth'
    cloudimgpath = r'data/dehaze/test1.png'
    clearingpath = r'data/dehaze/clear1.png'
    save_path = './results/test.png'
```

### Parameter Description å‚æ•°è¯´æ˜

- `--modelpath`: Path to the model weights. æ¨¡å‹æƒé‡è·¯å¾„ã€‚
- `--imgpath`: Path to the test image. æµ‹è¯•å›¾åƒè·¯å¾„ã€‚
- `--w`: The x-coordinate of the selected area. é€‰æ‹©åŒºåŸŸçš„ x åæ ‡ã€‚
- `--h`: The y-coordinate of the selected area. é€‰æ‹©åŒºåŸŸçš„ y åæ ‡ã€‚
- `--window_size`: The size of the selected area. åŒºåŸŸå¤§å°ã€‚
- `--fold`: The number of path integral steps. Higher values are closer to the true value. Default is 50, not recommended to modify. è·¯å¾„ç§¯åˆ†æ­¥æ•°ï¼Œè¶Šé«˜è¶Šæ¥è¿‘çœŸå®å€¼ï¼Œé»˜è®¤ 50ï¼Œä¸å»ºè®®ä¿®æ”¹ã€‚
- `--sigma`: Path integral parameter, not recommended to modify. è·¯å¾„ç§¯åˆ†å‚æ•°ï¼Œä¸å»ºè®®ä¿®æ”¹ã€‚
- `--l`: Path integral parameter, not recommended to modify. è·¯å¾„ç§¯åˆ†å‚æ•°ï¼Œä¸å»ºè®®ä¿®æ”¹ã€‚
- `--alpha`: Alpha value for blending. æ··åˆæ—¶çš„ alpha å€¼ã€‚
- `--zoomfactor`: å›¾åƒæ”¾å¤§å› å­ï¼Œé»˜è®¤1ï¼Œå¦‚æœä½ çš„ç”µè„‘GPUé…ç½®è¾ƒä½ï¼Œå¯é…Œæƒ…è®¾ç½®ä¸º4
- `--kde`: Whether to use KDE for visualization (requires high computational power). æ˜¯å¦ä½¿ç”¨ KDE è¿›è¡Œå¯è§†åŒ–ï¼ˆå¯¹ç”µè„‘æ€§èƒ½è¦æ±‚è¾ƒé«˜ï¼Œæ—¶é—´è¾ƒé•¿ï¼‰ã€‚
- `--output_dir`: Output image directory. è¾“å‡ºå›¾ç‰‡ç›®å½•ã€‚
- `--betterview`: æ”¹å–„å½’å› å›¾å¯è§†æ€§




## Code References ä»£ç ç¼–å†™å‚è€ƒ

æœ¬é¡¹ç›®åœ¨å¼€å‘è¿‡ç¨‹ä¸­å‚è€ƒäº†ä»¥ä¸‹å¼€æºé¡¹ç›®å¹¶å—å¯å‘ï¼Œæ„Ÿè°¢ä»–ä»¬å¯¹å¼€æºç¤¾åŒºçš„è´¡çŒ®ï¼š
- [LAM](https://github.com/fengyanzi/Local-Attribution-Map-for-Super-Resolution)

## Citation å¼•ç”¨

If you use DAM, please cite:
åœ¨ä½¿ç”¨ DAM æ—¶ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹æ–‡ç« ,è¯šæŒšæ„Ÿè°¢ï¼š

```bibtex
@article{chen2025tokenize,
  title={Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images},
  author={Chen, Jiuchen and Yan, Xinyu and Xu, Qizhi and Li, Kaiqi},
  journal={arXiv preprint arXiv:2504.09621},
  year={2025}
}
```
 -->
